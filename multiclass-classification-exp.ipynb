{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":393551,"sourceType":"datasetVersion","datasetId":174293}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import the necessay libraries\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nimport imageio\nimport urllib\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T14:19:53.011469Z","iopub.execute_input":"2024-11-21T14:19:53.012168Z","iopub.status.idle":"2024-11-21T14:19:53.018580Z","shell.execute_reply.started":"2024-11-21T14:19:53.012125Z","shell.execute_reply":"2024-11-21T14:19:53.017200Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Set up Directory","metadata":{}},{"cell_type":"code","source":"# setting up directory \n#file_name = '/kaggle/input/horses-or-humans-dataset'\ntraining = '../input/rock-paper-scissors-dataset/Rock-Paper-Scissors/train'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T14:19:55.239291Z","iopub.execute_input":"2024-11-21T14:19:55.239662Z","iopub.status.idle":"2024-11-21T14:19:55.244869Z","shell.execute_reply.started":"2024-11-21T14:19:55.239629Z","shell.execute_reply":"2024-11-21T14:19:55.243730Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# set up an image generator\ntraining_datagen = ImageDataGenerator(\n    rescale = 1/255,\n    rotation_range = 35,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    fill_mode = 'nearest'\n)\n\n# set up a train generator from the image generator\ntrain_gen = training_datagen.flow_from_directory(\n    training,\n    target_size = (150, 150),\n    class_mode = 'categorical'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T14:19:57.139600Z","iopub.execute_input":"2024-11-21T14:19:57.140047Z","iopub.status.idle":"2024-11-21T14:20:05.730107Z","shell.execute_reply.started":"2024-11-21T14:19:57.139982Z","shell.execute_reply":"2024-11-21T14:20:05.729074Z"}},"outputs":[{"name":"stdout","text":"Found 2520 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#initialize the keras model\nmodel = tf.keras.models.Sequential([\n     # Note the input shape is the desired size of the image: \n     # 150x150 with 3 bytes color\n     # This is the first convolution\n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu',\n                          input_shape = (150, 150, 3)),\n\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # second convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # third convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # fourth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2, 2), \n    # flatten the results to feed to DN\n    tf.keras.layers.Flatten(),\n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(512, activation = 'relu'),\n    tf.keras.layers.Dense(3, activation = 'softmax')  \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T14:20:18.401631Z","iopub.execute_input":"2024-11-21T14:20:18.402083Z","iopub.status.idle":"2024-11-21T14:20:18.490830Z","shell.execute_reply.started":"2024-11-21T14:20:18.402044Z","shell.execute_reply":"2024-11-21T14:20:18.489415Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# compile the model\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop',\n             metrics = ['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T14:23:33.540096Z","iopub.execute_input":"2024-11-21T14:23:33.541208Z","iopub.status.idle":"2024-11-21T14:23:33.562153Z","shell.execute_reply.started":"2024-11-21T14:23:33.541167Z","shell.execute_reply":"2024-11-21T14:23:33.560974Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# set up the test path directory\ntest = '../input/rock-paper-scissors-dataset/Rock-Paper-Scissors/test'\n\n# set up an image generator for validation data\ntest_datagen = ImageDataGenerator(\n    rescale = 1/255,\n    rotation_range = 35,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    fill_mode = 'nearest'\n)\n\n# set up a validation generator from the image generator\ntest_gen = test_datagen.flow_from_directory(\n    test,\n    target_size = (150, 150),\n    class_mode = 'categorical'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T14:31:07.731889Z","iopub.execute_input":"2024-11-21T14:31:07.732286Z","iopub.status.idle":"2024-11-21T14:31:09.041640Z","shell.execute_reply.started":"2024-11-21T14:31:07.732254Z","shell.execute_reply":"2024-11-21T14:31:09.040687Z"}},"outputs":[{"name":"stdout","text":"Found 372 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_gen, epochs = 25,\n                   validation_data = test_gen, verbose = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T14:35:09.812667Z","iopub.execute_input":"2024-11-21T14:35:09.813119Z","iopub.status.idle":"2024-11-21T15:26:26.610170Z","shell.execute_reply.started":"2024-11-21T14:35:09.813078Z","shell.execute_reply":"2024-11-21T15:26:26.608910Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.3624 - loss: 1.1595 - val_accuracy: 0.3387 - val_loss: 1.2002\nEpoch 2/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.5355 - loss: 0.9775 - val_accuracy: 0.6183 - val_loss: 0.8372\nEpoch 3/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.7419 - loss: 0.6271 - val_accuracy: 0.8414 - val_loss: 0.3988\nEpoch 4/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.8697 - loss: 0.3416 - val_accuracy: 0.8306 - val_loss: 0.4515\nEpoch 5/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9164 - loss: 0.2493 - val_accuracy: 0.8737 - val_loss: 0.3042\nEpoch 6/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - accuracy: 0.9242 - loss: 0.2250 - val_accuracy: 0.9435 - val_loss: 0.1752\nEpoch 7/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 1s/step - accuracy: 0.9366 - loss: 0.1876 - val_accuracy: 0.8952 - val_loss: 0.2544\nEpoch 8/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.1288 - val_accuracy: 0.7661 - val_loss: 0.7347\nEpoch 9/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1s/step - accuracy: 0.9532 - loss: 0.1278 - val_accuracy: 0.9274 - val_loss: 0.1936\nEpoch 10/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.9728 - loss: 0.0921 - val_accuracy: 0.9301 - val_loss: 0.2071\nEpoch 11/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9610 - loss: 0.1053 - val_accuracy: 0.9194 - val_loss: 0.2506\nEpoch 12/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9615 - loss: 0.1138 - val_accuracy: 0.9597 - val_loss: 0.1714\nEpoch 13/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.9707 - loss: 0.0600 - val_accuracy: 0.9328 - val_loss: 0.2022\nEpoch 14/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9789 - loss: 0.0597 - val_accuracy: 0.9435 - val_loss: 0.1950\nEpoch 15/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.9736 - loss: 0.0758 - val_accuracy: 0.9005 - val_loss: 0.3211\nEpoch 16/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.9803 - loss: 0.0665 - val_accuracy: 0.9409 - val_loss: 0.1265\nEpoch 17/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.9853 - loss: 0.0470 - val_accuracy: 0.9651 - val_loss: 0.0999\nEpoch 18/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.9747 - loss: 0.0875 - val_accuracy: 0.9194 - val_loss: 0.2199\nEpoch 19/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9789 - loss: 0.0579 - val_accuracy: 0.9382 - val_loss: 0.1536\nEpoch 20/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.9848 - loss: 0.0460 - val_accuracy: 0.9032 - val_loss: 0.2910\nEpoch 21/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.9865 - loss: 0.0426 - val_accuracy: 0.9382 - val_loss: 0.1955\nEpoch 22/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9897 - loss: 0.0391 - val_accuracy: 0.9462 - val_loss: 0.1392\nEpoch 23/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.9806 - loss: 0.0708 - val_accuracy: 0.9435 - val_loss: 0.1438\nEpoch 24/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.9885 - loss: 0.0377 - val_accuracy: 0.9409 - val_loss: 0.1766\nEpoch 25/25\n\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9914 - loss: 0.0281 - val_accuracy: 0.9651 - val_loss: 0.0992\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}