{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":393587,"sourceType":"datasetVersion","datasetId":174300},{"sourceId":9945631,"sourceType":"datasetVersion","datasetId":6115644},{"sourceId":9962746,"sourceType":"datasetVersion","datasetId":6128307}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n## You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:37:43.004080Z","iopub.execute_input":"2024-11-18T21:37:43.004483Z","iopub.status.idle":"2024-11-18T21:37:43.387271Z","shell.execute_reply.started":"2024-11-18T21:37:43.004449Z","shell.execute_reply":"2024-11-18T21:37:43.386378Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# import the necessay libraries\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nimport imageio\nimport urllib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T13:15:34.111305Z","iopub.execute_input":"2024-11-20T13:15:34.111702Z","iopub.status.idle":"2024-11-20T13:15:34.116757Z","shell.execute_reply.started":"2024-11-20T13:15:34.111668Z","shell.execute_reply":"2024-11-20T13:15:34.115793Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# setting up directory \n#file_name = '/kaggle/input/horses-or-humans-dataset'\ntraining = '../input/horses-or-humans-dataset/horse-or-human/train/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:37:58.185246Z","iopub.execute_input":"2024-11-18T21:37:58.186265Z","iopub.status.idle":"2024-11-18T21:37:58.190719Z","shell.execute_reply.started":"2024-11-18T21:37:58.186226Z","shell.execute_reply":"2024-11-18T21:37:58.189494Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# unzipping the zip folder\n#zip_ref = zipfile.ZipFile(file_name, 'r')\n#zip_ref.extractall(training)\n#zip_ref.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:37:59.681985Z","iopub.execute_input":"2024-11-18T21:37:59.682390Z","iopub.status.idle":"2024-11-18T21:37:59.686815Z","shell.execute_reply.started":"2024-11-18T21:37:59.682356Z","shell.execute_reply":"2024-11-18T21:37:59.685669Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# load the data using ImageDataGenerator\n# all images will be rescaled by 1.0/255\ntrain_datagen = ImageDataGenerator(rescale = 1/255)\n\ntrain_generator = train_datagen.flow_from_directory(training,\n                                                   target_size = (300, 300),\n                                                   class_mode = 'binary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:38:03.275135Z","iopub.execute_input":"2024-11-18T21:38:03.275915Z","iopub.status.idle":"2024-11-18T21:38:04.531911Z","shell.execute_reply.started":"2024-11-18T21:38:03.275867Z","shell.execute_reply":"2024-11-18T21:38:04.530881Z"}},"outputs":[{"name":"stdout","text":"Found 1027 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"There are several major differences between this dataset and the Fashion MNIST one\r\nthat you have to take into account when designing an architecture for classifying the\r\nimages. First, the images are much larger—300 × 300 pixels—so more layers may be\r\nneeded. Second, the images are full color, not grayscale, so each image will have three\r\nchannels instead of one. Third, there are only two image types, so we have a binary\r\nclassifier that can be implemented using just a single output neuron, where it\r\napproaches 0 for one class and 1 for the other. Keep these considerations in mind\r\nwhen exploring this archit\n\nThere are a number of things to note here. First of all, this is the very first layer. We’re\r\ndefining 16 filters, each 3 × 3, but the input shape of the image is (300, 300, 3).\r\nRemember that this is because our input image is 300 × 300 and it’s in color, so there\r\nare three channels, instead of just one for the monochrome Fashion MNIST dataset\r\nwe were using earlierecture:","metadata":{}},{"cell_type":"code","source":"# build the model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3,3), activation ='relu',\n                          input_shape = (300, 300, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation = 'relu'),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:38:07.090123Z","iopub.execute_input":"2024-11-18T21:38:07.091075Z","iopub.status.idle":"2024-11-18T21:38:07.226485Z","shell.execute_reply.started":"2024-11-18T21:38:07.091028Z","shell.execute_reply":"2024-11-18T21:38:07.225387Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# model summary\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:38:08.531343Z","iopub.execute_input":"2024-11-18T21:38:08.532311Z","iopub.status.idle":"2024-11-18T21:38:08.562206Z","shell.execute_reply.started":"2024-11-18T21:38:08.532269Z","shell.execute_reply":"2024-11-18T21:38:08.561203Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m298\u001b[0m, \u001b[38;5;34m298\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m149\u001b[0m, \u001b[38;5;34m149\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m147\u001b[0m, \u001b[38;5;34m147\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m4,640\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,606,144\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">147</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">147</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,606,144</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,704,097\u001b[0m (6.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,704,097</span> (6.50 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,704,097\u001b[0m (6.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,704,097</span> (6.50 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"To train the network, we’ll have to compile it with a loss function and an optimizer. In\r\nthis case the loss function can be binary cross entropy loss function binary cross\r\nentropy, because there are only two classes, and as the name suggests this is a loss\r\nfunction that is designed for that scenario. And we can try a new optimizer, root mean\r\nsquare propagation (RMSprop), that takes a learning rate (lr) parameter that allows us\r\nto tweak the learning.","metadata":{}},{"cell_type":"code","source":"# compile the CNN network\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001),\n             metrics = ['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:38:12.322645Z","iopub.execute_input":"2024-11-18T21:38:12.323038Z","iopub.status.idle":"2024-11-18T21:38:12.339073Z","shell.execute_reply.started":"2024-11-18T21:38:12.323004Z","shell.execute_reply":"2024-11-18T21:38:12.338019Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Over just 15 epochs, this architecture gives us a very impressive 100% accuracy on\r\nthe training set. Of course, this is just with the training data, and isn’t a  indication o \r\nperformance on data that the network hasn’t previously see This performance is an indication tha the model has an overfitting on the train data.. we will need to eavluate it on the test data to see it how it performs on unseen datan.\r","metadata":{}},{"cell_type":"code","source":"# train on the train generator\nhistory = model.fit(\n    train_generator,\n    epochs = 15\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T10:17:49.585027Z","iopub.execute_input":"2024-11-14T10:17:49.586117Z","iopub.status.idle":"2024-11-14T10:32:57.066145Z","shell.execute_reply.started":"2024-11-14T10:17:49.586066Z","shell.execute_reply":"2024-11-14T10:32:57.063721Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.5717 - loss: 0.6980\nEpoch 2/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9042 - loss: 0.3277\nEpoch 3/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - accuracy: 0.9393 - loss: 0.2321\nEpoch 4/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9656 - loss: 0.1072\nEpoch 5/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9195 - loss: 0.6513\nEpoch 6/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9891 - loss: 0.0593\nEpoch 7/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.9951 - loss: 0.0178\nEpoch 8/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9845 - loss: 0.0699\nEpoch 9/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - accuracy: 0.9401 - loss: 0.2343\nEpoch 10/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.9955 - loss: 0.0274\nEpoch 11/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9901 - loss: 0.0432\nEpoch 12/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9955 - loss: 0.0197\nEpoch 13/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.8619e-04\nEpoch 14/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 9.6098e-05\nEpoch 15/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.4411e-05\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Adding Validation to the Horses or Humans Dataset","metadata":{}},{"cell_type":"code","source":"# get the validation data\nvalidation_dir = '../input/horses-or-humans-dataset/horse-or-human/validation/'\n\n# set up the image generator for validation\nvalidation_datagen = ImageDataGenerator(rescale= 1/255)\nvalidation_gen = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (300, 300),\n    class_mode = 'binary'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:38:16.385783Z","iopub.execute_input":"2024-11-18T21:38:16.386135Z","iopub.status.idle":"2024-11-18T21:38:16.651272Z","shell.execute_reply.started":"2024-11-18T21:38:16.386105Z","shell.execute_reply":"2024-11-18T21:38:16.650422Z"}},"outputs":[{"name":"stdout","text":"Found 256 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"To have TensorFlow perform the validation for you, you simply update your \nmodel.fir method to indicate that you want to use the validation data t \r\ntest the model epoch by epoch. You do this by using the validation_data paramet r\r\nand passing it the validation generator you just construct\n\nAfter training for 15 epochs, you should see that your model is 100%+ accurate on the\r\ntraining set, but only about 88% on the validation set. This is an indication that the\r\nmodel is overfitting, as we saw in the previous chapter.\red:","metadata":{}},{"cell_type":"code","source":"# validate the model on the validation set\nhistory = model.fit(\n    train_generator,\n    epochs = 15,\n    validation_data = validation_gen\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:38:19.362081Z","iopub.execute_input":"2024-11-18T21:38:19.362503Z","iopub.status.idle":"2024-11-18T21:54:12.904247Z","shell.execute_reply.started":"2024-11-18T21:38:19.362469Z","shell.execute_reply":"2024-11-18T21:54:12.903146Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.5899 - loss: 0.6943 - val_accuracy: 0.8281 - val_loss: 0.3740\nEpoch 2/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.8195 - loss: 0.4499 - val_accuracy: 0.8320 - val_loss: 0.9249\nEpoch 3/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.8934 - loss: 0.3929 - val_accuracy: 0.7930 - val_loss: 0.7396\nEpoch 4/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9548 - loss: 0.1807 - val_accuracy: 0.8945 - val_loss: 0.6588\nEpoch 5/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9663 - loss: 0.1115 - val_accuracy: 0.8984 - val_loss: 0.7605\nEpoch 6/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.9830 - loss: 0.0482 - val_accuracy: 0.8789 - val_loss: 1.1052\nEpoch 7/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.9641 - loss: 0.1315 - val_accuracy: 0.8633 - val_loss: 1.1555\nEpoch 8/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.9796 - loss: 0.2104 - val_accuracy: 0.8945 - val_loss: 1.0239\nEpoch 9/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.9916 - loss: 0.0223 - val_accuracy: 0.9297 - val_loss: 0.4166\nEpoch 10/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9300 - loss: 1.5177 - val_accuracy: 0.8477 - val_loss: 1.6024\nEpoch 11/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9935 - loss: 0.0309 - val_accuracy: 0.8477 - val_loss: 0.9630\nEpoch 12/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.9945 - loss: 0.0209 - val_accuracy: 0.8789 - val_loss: 1.3647\nEpoch 13/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9944 - loss: 0.0264 - val_accuracy: 0.8633 - val_loss: 2.0510\nEpoch 14/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.8984 - val_loss: 1.4592\nEpoch 15/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.9824 - loss: 0.0737 - val_accuracy: 0.8750 - val_loss: 1.8922\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Testing the Model","metadata":{}},{"cell_type":"markdown","source":"There are a few important points to consider here. First, even though the network\r\nwas trained on synthetic, computer-generated imagery, itstill  performs quite well at spo\r\nting horsebut is not able to identifying humans in real photographs.s. This is a potential boon in that you m or may y\r\nnot need thousands of photographs to train a model, and can do it relatively chea ly\r\nwith  But this dataset also demonstrates a fundamental issue you will face. Your training set\ncannot hope to represent every possible scenario your model might face in the wild \r\nand thus the model will always have some level of overspecializati n toward the trn‐\r\ning setCGI.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\n\n# Define the path to the folder containing your images and the model path\nimage_folder_path = '../input/picture-model/'\n\n\n# Set the image size to match the input size of the model\nimg_size = (300, 300, 3)  # Change this based on your model's input size\n\n# Loop through each image in the folder\nfor img_name in os.listdir(image_folder_path):\n    img_path = os.path.join(image_folder_path, img_name)\n    \n    # Load and preprocess the image\n    img = image.load_img(img_path, target_size=img_size)\n    img_array = image.img_to_array(img) / 255.0  # Normalize if required by your model\n    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n\n    # Predict the class\n    prediction = model.predict(img_array)\n    \n    # Get the prediction result\n    if classes[0]>0.5:\n         print(img_name + \" is a human\")\n    else:\n         print(img_name + \" is a horse\")\n    \n    #print(f\"Image: {img_name} - Prediction: {class_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:56:28.063245Z","iopub.execute_input":"2024-11-18T22:56:28.063649Z","iopub.status.idle":"2024-11-18T22:56:28.733768Z","shell.execute_reply.started":"2024-11-18T22:56:28.063614Z","shell.execute_reply":"2024-11-18T22:56:28.732709Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\nbird.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\npic_two.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\nhorse_two.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\nhorse_one.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\npic_three.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\npic_one.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\nhorse_three.jpg is a horse\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"## Image Augmentation\n\nIn the previous section, you built a horse-or-human classifier model that was trained \non a relatively small dataset. As a result, you soon began to hit problems classifyin \r\nsome previously unseen images, such as t e miscategorization of humanan with a\r\nhorse because the training set didn’t include any images of people in that pose","metadata":{}},{"cell_type":"code","source":"# add extra transformation to the image generator\ntrain_datagen = ImageDataGenerator(rescale = 1/255,\n                                  rotation_range = 40,\n                                  width_shift_range = 0.2,\n                                  height_shift_range = 0.2,\n                                  shear_range = 0.2,\n                                  horizontal_flip = True,\n                                  fill_mode = 'nearest')\n\ntrain_generator = train_datagen.flow_from_directory(training,\n                                                   target_size = (300, 300),\n                                                   class_mode = 'binary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:08:34.708296Z","iopub.execute_input":"2024-11-18T23:08:34.709229Z","iopub.status.idle":"2024-11-18T23:08:36.052716Z","shell.execute_reply.started":"2024-11-18T23:08:34.709190Z","shell.execute_reply":"2024-11-18T23:08:36.051811Z"}},"outputs":[{"name":"stdout","text":"Found 1027 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# retrain the train image and compare to validation set\nhistory = model.fit(\n    train_generator,\n    epochs = 15,\n    validation_data = validation_gen\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:13:15.827994Z","iopub.execute_input":"2024-11-18T23:13:15.828855Z","iopub.status.idle":"2024-11-18T23:31:47.346488Z","shell.execute_reply.started":"2024-11-18T23:13:15.828812Z","shell.execute_reply":"2024-11-18T23:31:47.345549Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.7798 - loss: 0.7409 - val_accuracy: 0.8281 - val_loss: 1.0970\nEpoch 2/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.8605 - loss: 0.3377 - val_accuracy: 0.8984 - val_loss: 0.7509\nEpoch 3/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - accuracy: 0.8854 - loss: 0.3319 - val_accuracy: 0.8633 - val_loss: 1.1241\nEpoch 4/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - accuracy: 0.8978 - loss: 0.2495 - val_accuracy: 0.8945 - val_loss: 0.5514\nEpoch 5/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.9356 - loss: 0.1834 - val_accuracy: 0.8086 - val_loss: 2.0778\nEpoch 6/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.9179 - loss: 0.2028 - val_accuracy: 0.8320 - val_loss: 1.0879\nEpoch 7/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.9542 - loss: 0.1278 - val_accuracy: 0.7266 - val_loss: 3.3469\nEpoch 8/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - accuracy: 0.9442 - loss: 0.1605 - val_accuracy: 0.8320 - val_loss: 1.2057\nEpoch 9/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.9583 - loss: 0.1089 - val_accuracy: 0.8789 - val_loss: 1.1638\nEpoch 10/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - accuracy: 0.9586 - loss: 0.1219 - val_accuracy: 0.8203 - val_loss: 1.7635\nEpoch 11/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.9716 - loss: 0.0765 - val_accuracy: 0.8242 - val_loss: 1.7785\nEpoch 12/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - accuracy: 0.9824 - loss: 0.0777 - val_accuracy: 0.8203 - val_loss: 1.4677\nEpoch 13/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.9744 - loss: 0.1067 - val_accuracy: 0.8164 - val_loss: 1.8496\nEpoch 14/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - accuracy: 0.9748 - loss: 0.1010 - val_accuracy: 0.8281 - val_loss: 2.1309\nEpoch 15/15\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.9526 - loss: 0.2910 - val_accuracy: 0.8633 - val_loss: 1.5923\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\n#from tensorflow.keras.models import load_model\n\n# Define the path to the folder containing your images and the model path\nimage_folder_path = '../input/picture-model/'\n\n\n# Set the image size to match the input size of the model\nimg_size = (300, 300, 3)  # Change this based on your model's input size\n\n# Loop through each image in the folder\nfor img_name in os.listdir(image_folder_path):\n    img_path = os.path.join(image_folder_path, img_name)\n    \n    # Load and preprocess the image\n    img = image.load_img(img_path, target_size=img_size)\n    img_array = image.img_to_array(img) / 255.0  # Normalize if required by your model\n    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n\n    # Predict the class\n    prediction = model.predict(img_array)\n    \n    # Get the prediction result\n    if classes[0]>0.5:\n         print(img_name + \" is a human\")\n    else:\n         print(img_name + \" is a horse\")\n    \n    #print(f\"Image: {img_name} - Prediction: {class_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:31:47.348454Z","iopub.execute_input":"2024-11-18T23:31:47.349459Z","iopub.status.idle":"2024-11-18T23:31:48.024260Z","shell.execute_reply.started":"2024-11-18T23:31:47.349420Z","shell.execute_reply":"2024-11-18T23:31:48.023158Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\nbird.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\npic_two.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\nhorse_two.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\nhorse_one.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\npic_three.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\npic_one.jpg is a horse\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\nhorse_three.jpg is a horse\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transfer Learning","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T13:09:08.185331Z","iopub.execute_input":"2024-11-20T13:09:08.185842Z","iopub.status.idle":"2024-11-20T13:09:08.196024Z","shell.execute_reply.started":"2024-11-20T13:09:08.185795Z","shell.execute_reply":"2024-11-20T13:09:08.195009Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import h5py\n\n# get the weights of the model url\nweights_url = \"../input/inception-v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n\nweights_file = h5py.File(weights_url, \"r\")\nweights_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T13:36:21.996357Z","iopub.execute_input":"2024-11-20T13:36:21.996756Z","iopub.status.idle":"2024-11-20T13:36:22.013191Z","shell.execute_reply.started":"2024-11-20T13:36:21.996721Z","shell.execute_reply":"2024-11-20T13:36:22.012206Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<HDF5 file \"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\" (mode r)>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}